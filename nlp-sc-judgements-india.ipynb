{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac30bb2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T10:41:14.091885Z",
     "iopub.status.busy": "2025-09-03T10:41:14.091217Z",
     "iopub.status.idle": "2025-09-03T10:41:22.331950Z",
     "shell.execute_reply": "2025-09-03T10:41:22.330383Z"
    },
    "papermill": {
     "duration": 8.2464,
     "end_time": "2025-09-03T10:41:22.334109",
     "exception": false,
     "start_time": "2025-09-03T10:41:14.087709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting virtualenv\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading virtualenv-20.34.0-py3-none-any.whl.metadata (4.6 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting distlib<1,>=0.3.7 (from virtualenv)\r\n",
      "  Downloading distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.11/dist-packages (from virtualenv) (3.18.0)\r\n",
      "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from virtualenv) (4.3.8)\r\n",
      "Downloading virtualenv-20.34.0-py3-none-any.whl (6.0 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/6.0 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading distlib-0.4.0-py2.py3-none-any.whl (469 kB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/469.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: distlib, virtualenv\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed distlib-0.4.0 virtualenv-20.34.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created virtual environment CPython3.11.13.final.0-64 in 794ms\r\n",
      "  creator CPython3Posix(dest=/kaggle/working/venv, clear=False, no_vcs_ignore=False, global=False)\r\n",
      "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\r\n",
      "    added seed packages: pip==25.2, setuptools==80.9.0\r\n",
      "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\r\n"
     ]
    }
   ],
   "source": [
    "!pip install virtualenv\n",
    "!virtualenv venv\n",
    "!source venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab10bc52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T10:41:22.341221Z",
     "iopub.status.busy": "2025-09-03T10:41:22.340834Z",
     "iopub.status.idle": "2025-09-03T10:41:48.453373Z",
     "shell.execute_reply": "2025-09-03T10:41:48.451081Z"
    },
    "papermill": {
     "duration": 26.118684,
     "end_time": "2025-09-03T10:41:48.455719",
     "exception": false,
     "start_time": "2025-09-03T10:41:22.337035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF==1.24.10\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading PyMuPDF-1.24.10-cp311-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tools\r\n",
      "  Downloading tools-1.0.5-py3-none-any.whl.metadata (1.3 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\r\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\r\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\r\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\r\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.1\r\n",
      "  Downloading numpy-1.26.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy==1.11.2\r\n",
      "  Downloading scipy-1.11.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (59 kB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/59.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDFb==1.24.10 (from PyMuPDF==1.24.10)\r\n",
      "  Downloading PyMuPDFb-1.24.10-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\r\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\r\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\r\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\r\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\r\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\r\n",
      "INFO: pip is looking at multiple versions of thinc to determine which version is compatible with other requirements. This could take a while.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading thinc-8.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading blis-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\r\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\r\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\r\n",
      "Downloading PyMuPDF-1.24.10-cp311-none-manylinux2014_x86_64.whl (3.5 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m155.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading numpy-1.26.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/18.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/18.2 MB\u001b[0m \u001b[31m224.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m14.0/18.2 MB\u001b[0m \u001b[31m193.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m15.4/18.2 MB\u001b[0m \u001b[31m125.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading scipy-1.11.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/36.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/36.3 MB\u001b[0m \u001b[31m199.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/36.3 MB\u001b[0m \u001b[31m166.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.3/36.3 MB\u001b[0m \u001b[31m121.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/36.3 MB\u001b[0m \u001b[31m132.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/36.3 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.2/36.3 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/36.3 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.4/36.3 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/36.3 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m27.5/36.3 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m28.7/36.3 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m34.2/36.3 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m36.2/36.3 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m36.2/36.3 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m36.2/36.3 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m36.2/36.3 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading PyMuPDFb-1.24.10-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.9 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/15.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/15.9 MB\u001b[0m \u001b[31m146.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m12.2/15.9 MB\u001b[0m \u001b[31m185.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m218.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m218.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tools-1.0.5-py3-none-any.whl (40 kB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading thinc-8.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m247.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading blis-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/11.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/11.7 MB\u001b[0m \u001b[31m185.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m178.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m178.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building wheels for collected packages: rouge-score\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=b11001be9018391406061fd7f6d3c1274b05f8668b79fa15eb482b0b0e4a1484\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\r\n",
      "Successfully built rouge-score\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: tools, PyMuPDFb, numpy, scipy, rouge-score, PyMuPDF, blis, thinc\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 1.26.4\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Uninstalling numpy-1.26.4:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled numpy-1.26.4\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: scipy\r\n",
      "    Found existing installation: scipy 1.15.3\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Uninstalling scipy-1.15.3:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled scipy-1.15.3\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: blis\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found existing installation: blis 1.3.0\r\n",
      "    Uninstalling blis-1.3.0:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled blis-1.3.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: thinc\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found existing installation: thinc 8.3.6\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Uninstalling thinc-8.3.6:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled thinc-8.3.6\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
      "mkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.26.1 which is incompatible.\r\n",
      "mkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.26.1 which is incompatible.\r\n",
      "mkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.26.1 which is incompatible.\r\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\r\n",
      "onnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.1 which is incompatible.\r\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\r\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\r\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\r\n",
      "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.2 which is incompatible.\r\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\r\n",
      "pandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.11.2 which is incompatible.\r\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\r\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\r\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\r\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed PyMuPDF-1.24.10 PyMuPDFb-1.24.10 blis-1.2.1 numpy-1.26.1 rouge-score-0.1.2 scipy-1.11.2 thinc-8.3.4 tools-1.0.5\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyMuPDF==1.24.10 tools transformers nltk spacy pandas tqdm sentencepiece rouge-score numpy==1.26.1 scipy==1.11.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0cf7066",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-03T10:41:48.473120Z",
     "iopub.status.busy": "2025-09-03T10:41:48.472807Z",
     "iopub.status.idle": "2025-09-03T10:42:00.687511Z",
     "shell.execute_reply": "2025-09-03T10:42:00.686279Z"
    },
    "papermill": {
     "duration": 12.226336,
     "end_time": "2025-09-03T10:42:00.690143",
     "exception": false,
     "start_time": "2025-09-03T10:41:48.463807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        #print(os.path.join(dirname, filename))\n",
    "        os.path.join(dirname, filename)\n",
    "\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5062396",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T10:42:00.708087Z",
     "iopub.status.busy": "2025-09-03T10:42:00.707593Z",
     "iopub.status.idle": "2025-09-03T10:43:12.511302Z",
     "shell.execute_reply": "2025-09-03T10:43:12.510193Z"
    },
    "papermill": {
     "duration": 71.814315,
     "end_time": "2025-09-03T10:43:12.512936",
     "exception": false,
     "start_time": "2025-09-03T10:42:00.698621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 10:42:26.675515: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756896146.921503      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756896146.994817      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:2025-09-03 10:43:02,760:jax._src.xla_bridge:924: Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:2025-09-03 10:43:02,785:jax._src.xla_bridge:924: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TAPAS models are not usable since `tensorflow_probability` can't be loaded. It seems you have `tensorflow_probability` installed with the wrong tensorflow version. Please try to reinstall it following the instructions here: https://github.com/tensorflow/probability.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GroupViT models are not usable since `tensorflow_probability` can't be loaded. It seems you have `tensorflow_probability` installed with the wrong tensorflow version.Please try to reinstall it following the instructions here: https://github.com/tensorflow/probability.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # this is PyMuPDF\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import torch\n",
    "import nltk\n",
    "from torch.optim import AdamW\n",
    "from transformers import *\n",
    "import spacy\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Ensure nltk data is downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load or download spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "    import spacy.cli\n",
    "    spacy.cli.download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d366579",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T10:43:12.527443Z",
     "iopub.status.busy": "2025-09-03T10:43:12.526528Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-09-03T10:43:12.519119",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10cf559439cd456697b351f7eaf15ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46cfb17d292f48f0a3962403ab0155a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-base/snapshots/a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1/spiece.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file added_tokens.json from cache at None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file special_tokens_map.json from cache at None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer_config.json from cache at None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-base/snapshots/a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1/tokenizer.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b060704fa8cc404d9d763e4ca0c8b025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-base/snapshots/a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-base/snapshots/a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320a018a5b4047a796afda5b2bf54db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--t5-base/snapshots/a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1/model.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144767796ac6418d808f8c0d4400b8bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--t5-base/snapshots/a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1/generation_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Scanning all PDFs under: /kaggle/input/legal-dataset-sc-judgments-india-19502024/supreme_court_judgments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Cleaning text...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Simplifying text (this may take time, please wait)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Simplifying:   0%|          | 0/26688 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Simplifying:   0%|          | 1/26688 [08:17<3689:18:00, 497.68s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Simplifying:   0%|          | 2/26688 [24:06<5656:30:14, 763.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Simplifying:   0%|          | 3/26688 [28:51<4037:40:49, 544.71s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Simplifying:   0%|          | 4/26688 [34:11<3379:46:58, 455.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Simplifying:   0%|          | 5/26688 [1:09:40<7852:06:19, 1059.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Simplifying:   0%|          | 6/26688 [1:14:42<5942:07:15, 801.73s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Simplifying:   0%|          | 7/26688 [1:29:17<6120:42:56, 825.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Simplifying:   0%|          | 8/26688 [1:41:44<5934:33:00, 800.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Simplifying:   0%|          | 9/26688 [1:48:35<5031:24:15, 678.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Simplifying:   0%|          | 10/26688 [1:55:49<4471:01:55, 603.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Simplifying:   0%|          | 11/26688 [2:19:11<6281:06:53, 847.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Simplifying:   0%|          | 12/26688 [2:24:25<5077:26:53, 685.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Simplifying:   0%|          | 13/26688 [2:25:45<3720:20:07, 502.09s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Simplifying:   0%|          | 14/26688 [2:29:46<3135:16:07, 423.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Simplifying:   0%|          | 15/26688 [2:47:06<4512:13:13, 609.01s/it]"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# 🔹 Step 1: Load All PDFs Recursively\n",
    "# ==========================\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            return \" \".join([page.get_text() for page in doc])\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def load_pdf_dataset_recursive(root_dir):\n",
    "    if not os.path.exists(root_dir):\n",
    "        raise FileNotFoundError(f\"❌ Folder not found: {root_dir}\")\n",
    "    \n",
    "    data = []\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for file in filenames:\n",
    "            if file.lower().endswith(\".pdf\"):\n",
    "                file_path = os.path.join(dirpath, file)\n",
    "                text = extract_text_from_pdf(file_path)\n",
    "                if text.strip():\n",
    "                    data.append({\n",
    "                        \"filepath\": file_path,\n",
    "                        \"filename\": file,\n",
    "                        \"text\": text\n",
    "                    })\n",
    "    if not data:\n",
    "        raise ValueError(\"❌ No valid PDFs found in the folder structure.\")\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# ==========================\n",
    "# 🔹 Step 2: Clean Text\n",
    "# ==========================\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'Page \\d+', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# ==========================\n",
    "# 🔹 Step 3: Lexical Simplification\n",
    "# ==========================\n",
    "domain_glossary = {\n",
    "    \"plaintiff\": \"person filing the case\",\n",
    "    \"defendant\": \"accused person\",\n",
    "    \"jurisdiction\": \"legal authority\",\n",
    "    \"hereinafter\": \"from now on\",\n",
    "    \"hereby\": \"by this\",\n",
    "    \"forthwith\": \"immediately\",\n",
    "    \"notwithstanding\": \"despite\",\n",
    "    \"adjudicate\": \"decide\",\n",
    "    \"statutory\": \"legal\",\n",
    "    \"aforesaid\": \"mentioned earlier\",\n",
    "    \"writ\": \"written order\",\n",
    "    \"amicus curiae\": \"friend of the court\",\n",
    "    \"habeas corpus\": \"produce the body\",\n",
    "    \"in camera\": \"in private\",\n",
    "    \"res judicata\": \"already judged\",\n",
    "    \"ultra vires\": \"beyond legal power\",\n",
    "    \"inter alia\": \"among others\",\n",
    "    \"prima facie\": \"at first glance\",\n",
    "    \"sub judice\": \"under judicial consideration\",\n",
    "    \"respondent\": \"party replying in court\",\n",
    "    \"affidavit\": \"written sworn statement\",\n",
    "    \"testimony\": \"oral evidence\",\n",
    "    \"litigation\": \"legal case\",\n",
    "    \"contractual obligation\": \"agreed duty\",\n",
    "    \"breach of contract\": \"breaking the agreement\",\n",
    "    \"injunction\": \"court order to stop an action\",\n",
    "    \"appellant\": \"person who appeals\",\n",
    "    \"pro bono\": \"free legal work\",\n",
    "    \"verdict\": \"decision\",\n",
    "    \"liable\": \"legally responsible\",\n",
    "    \"tort\": \"civil wrongdoing\",\n",
    "    \"contempt of court\": \"disrespect to the court\",\n",
    "    \"perjury\": \"lying under oath\",\n",
    "    \"tribunal\": \"special legal court\",\n",
    "    \"remand\": \"send back for further action\",\n",
    "    \"acquittal\": \"not guilty decision\",\n",
    "    \"conviction\": \"guilty decision\",\n",
    "    \"indictment\": \"formal charge\",\n",
    "    \"precedent\": \"previous similar case\",\n",
    "    \"negligence\": \"failure to take care\",\n",
    "    \"hearsay\": \"second-hand information\",\n",
    "    \"disposition\": \"final settlement of a case\",\n",
    "    \"appellate court\": \"appeal court\",\n",
    "    \"statute of limitations\": \"legal time limit\",\n",
    "    \"grievance\": \"complaint\",\n",
    "    \"subpoena\": \"legal summons\",\n",
    "    \"decree\": \"court order\",\n",
    "    \"injured party\": \"person harmed\",\n",
    "    \"guardian ad litem\": \"court-appointed guardian\",\n",
    "    \"legal remedy\": \"legal solution or action\",\n",
    "    \"equity\": \"fairness in law\",\n",
    "    \"arbitration\": \"legal dispute resolution without court\",\n",
    "    \"mediation\": \"assisted negotiation\",\n",
    "    \"cross-examination\": \"questioning by the opposing side\"\n",
    "}\n",
    "\n",
    "def apply_lexical_simplification(text, glossary):\n",
    "    for complex_word, simple_word in glossary.items():\n",
    "        pattern = r'\\b' + re.escape(complex_word) + r'\\b'\n",
    "        text = re.sub(pattern, simple_word, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "# ==========================\n",
    "# 🔹 Step 4: Sentence Segmentation\n",
    "# ==========================\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "# ==========================\n",
    "# 🔹 Step 5: Load Transformer Model (T5)\n",
    "# ==========================\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "def simplify_with_t5(sentence):\n",
    "    input_text = \"simplify: \" + sentence\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    output_ids = model.generate(input_ids, max_length=150, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# ==========================\n",
    "# 🔹 Step 6: Rule-Based Syntactic Cleanup\n",
    "# ==========================\n",
    "def syntactic_cleanup(text):\n",
    "    text = text.replace(\"shall be\", \"will be\")\n",
    "    text = text.replace(\"shall not\", \"will not\")\n",
    "    text = text.replace(\"shall\", \"will\")\n",
    "    return text\n",
    "\n",
    "# ==========================\n",
    "# 🔹 Step 7: Full Simplification Pipeline\n",
    "# ==========================\n",
    "def domain_text_simplification_pipeline(text):\n",
    "    try:\n",
    "        text = clean_text(text)\n",
    "        text = apply_lexical_simplification(text, domain_glossary)\n",
    "        sentences = preprocess_text(text)\n",
    "        simplified_output = []\n",
    "\n",
    "        for sent in sentences:\n",
    "            try:\n",
    "                simplified = simplify_with_t5(sent)\n",
    "                simplified = syntactic_cleanup(simplified)\n",
    "                simplified_output.append(simplified)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error simplifying sentence: {e}\")\n",
    "                continue\n",
    "\n",
    "        return \" \".join(simplified_output)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error in pipeline: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# ==========================\n",
    "# 🔹 Step 8: Run on Entire Dataset (with tqdm progress bar)\n",
    "# ==========================\n",
    "dataset_path = \"/kaggle/input/legal-dataset-sc-judgments-india-19502024/supreme_court_judgments\"\n",
    "\n",
    "print(f\"📁 Scanning all PDFs under: {dataset_path}\")\n",
    "df = load_pdf_dataset_recursive(dataset_path)\n",
    "\n",
    "print(\"🧹 Cleaning text...\")\n",
    "df[\"cleaned_text\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "print(\"🔁 Simplifying text (this may take time, please wait)...\")\n",
    "simplified_texts = []\n",
    "for text in tqdm(df[\"cleaned_text\"], desc=\"Simplifying\"):\n",
    "    simplified_texts.append(domain_text_simplification_pipeline(text))\n",
    "df[\"simplified_text\"] = simplified_texts\n",
    "\n",
    "# ==========================\n",
    "# 🔹 Step 9: Save Results\n",
    "# ==========================\n",
    "output_path = \"simplified_judgments_recursive.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"✅ Saved simplified judgments to: {output_path}\")\n",
    "\n",
    "# Optional Preview\n",
    "print(\"\\n📜 Original Sample:\\n\", df.iloc[0][\"cleaned_text\"][:1000])\n",
    "print(\"\\n🪄 Simplified Sample:\\n\", df.iloc[0][\"simplified_text\"][:1000])\n",
    "\n",
    "# ==========================\n",
    "# 🔹 Step 10: Evaluation (BLEU and ROUGE)\n",
    "# ==========================\n",
    "def evaluate_simplification(reference_texts, predicted_texts):\n",
    "    # BLEU (sentence-level average)\n",
    "    bleu_scores = []\n",
    "    for ref, pred in zip(reference_texts, predicted_texts):\n",
    "        ref_tokens = [ref.split()]\n",
    "        pred_tokens = pred.split()\n",
    "        bleu = sentence_bleu(ref_tokens, pred_tokens)\n",
    "        bleu_scores.append(bleu)\n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "    # ROUGE-L (macro average)\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    rouge_l_f1_scores = []\n",
    "    for ref, pred in zip(reference_texts, predicted_texts):\n",
    "        scores = scorer.score(ref, pred)\n",
    "        rouge_l_f1_scores.append(scores['rougeL'].fmeasure)\n",
    "    avg_rouge_l = sum(rouge_l_f1_scores) / len(rouge_l_f1_scores)\n",
    "\n",
    "    print(f\"🔍 Evaluation metrics:\")\n",
    "    print(f\" - Average BLEU score: {avg_bleu:.4f}\")\n",
    "    print(f\" - Average ROUGE-L F1 score: {avg_rouge_l:.4f}\")\n",
    "\n",
    "# Example usage of evaluation (if you have reference simplified texts):\n",
    "# Here just as example, replace with your real reference and prediction lists\n",
    "# evaluate_simplification(list_of_references, list_of_predictions)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7218008,
     "sourceId": 11510985,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-03T10:41:08.230238",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}